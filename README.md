Enterprise Document Intelligence â€“ RAG Chatbot

A production-style Retrieval-Augmented Generation (RAG) system that enables natural language querying across multiple enterprise PDF documents without manual document selection. The system automatically retrieves relevant information from the best sources and generates grounded answers with full observability.

ğŸš€ Features:

Multi-document semantic search across large PDFs

Automatic document selection (no file input required)

Optimized PDF ingestion and semantic chunking

FAISS-based vector search for fast retrieval

Chat-style UI with session-level memory

LangSmith observability for tracing and debugging

ğŸ§  Problem Solved:

Enterprise documents are large, scattered, and time-consuming to search manually.
This system automates document understanding and retrieval, reducing information-search effort by ~60â€“70% while improving answer accuracy and traceability.

ğŸ› ï¸ Tech Stack:
Backend: FastAPI, LangChain, FAISS

Frontend: Streamlit

LLM & Observability: OpenAI, LangSmith

PDF Parsing: PyMuPDF, pdfplumber

ğŸ“¦ Clone the Repository
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name

â–¶ï¸ Run Locally
1ï¸âƒ£ Install dependencies
pip install -r requirements.txt

2ï¸âƒ£ Set environment variables (.env)
OPENAI_API_KEY=your_openai_key
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_key
LANGCHAIN_PROJECT=enterprise-document-rag

3ï¸âƒ£ Start FastAPI backend
uvicorn enterprise_document_intelligence_rag.app.main:app --reload

4ï¸âƒ£ Ingest documents (one-time)
Open:
http://127.0.0.1:8000/docs

Call:
POST /ingest

5ï¸âƒ£ Start Streamlit frontend
streamlit run streamlit_app.py

ğŸ“ˆ Impact

~60â€“70% reduction in manual document search effort

~40â€“50% faster debugging using LangSmith observability

Scales efficiently to large, multi-document corpora

ğŸ‘¤ Author

Asiya Irshad
Aspiring GenAI Engineer

ğŸ”— GitHub: [github.com/asiyairshad]

